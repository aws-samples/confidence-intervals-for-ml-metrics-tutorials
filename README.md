# Confidence Intervals for ML Performance Metrics

Assessing the uncertainty of performance metrics of machine learning (ML) models
is critical. By quantifying the uncertainty in our performance estimates, we can
better gauge the effectiveness of ML models on real-world customer data. This
repository provides brief tutorials on how to construct confidence intervals for
the performance metrics of machine learning models such as binary accurracy, log
loss, F1 score, AUC, MSE, and others.

## :rocket: Get started

To get started, open the
[classification-tasks.ipynb](classification-tasks.ipynb) notebook for
classification tasks.

<!-- , the [1-1-matching-tasks.ipynb](1-1-matching-tasks.ipynb)
notebook for 1:1 matching tasks (e.g., 1:1 facial verification), and the
[regression-tasks.ipynb](regression-tasks.ipynb) notebook for regression tasks.
The one on classification tasks is the most comprehensive one, so you may want
to start with that one. -->

## :books: Additional resources

If you are looking for more resources, consider consulting [Stack
Overflow](https://stackoverflow.com/),
[blogs](https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html),
[textbooks](https://hastie.su.domains/Papers/ESLII.pdf), and
[papers](https://projecteuclid.org/journals/statistical-science/volume-11/issue-3/Bootstrap-confidence-intervals/10.1214/ss/1032280214.full).

## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This library is licensed under the MIT-0 License. See the LICENSE file.
